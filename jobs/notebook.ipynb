{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "108\n"
          ]
        }
      ],
      "source": [
        "#you can find users here\n",
        "\n",
        "\n",
        "from pymongo import MongoClient\n",
        "client = MongoClient(\n",
        "    \"mongodb://mongo:xQxzXZEzUilnKKhrbELE@containers-us-west-114.railway.app:6200\"\n",
        ")\n",
        "database = client[\"users\"]\n",
        "collection = database[\"recipients\"]\n",
        "# first make a set to avoid getting identical phone numbers:\n",
        "phone_numbers = set()\n",
        "recipients = collection.find()\n",
        "for recipient in recipients:\n",
        "    phone_number = recipient[\"phone_number\"]\n",
        "    phone_numbers.add(phone_number)\n",
        "# convert back to list\n",
        "phone_numbers = list(phone_numbers)\n",
        "print(len(phone_numbers))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#create example vectors\n",
        "vector_embeddings = [\n",
        "[0.2] * 1536,\n",
        "[-0.3] * 1536,\n",
        "[0.7] * 1536,\n",
        "]\n",
        "# convert list of lists to list of vectors\n",
        "vectors = [pinecone.Vector(values=x, id=\"example\") for x in vector_embeddings]\n",
        "\n",
        "# upsert vectors to index\n",
        "#index.upsert(vectors=vectors, namespace=recipient)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for phone_number in phone_numbers:\n",
        "    index.upsert(vectors=vectors, namespace=phone_number)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pinecone\n",
        "from pymongo import MongoClient\n",
        "import os\n",
        "from langchain.vectorstores import Pinecone\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.memory import MongoDBChatMessageHistory\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "##########################       STEP ONE          ####################################\n",
        "########### Retrieve recipient phone numbers from MongoDB and add them to a list\n",
        "client = MongoClient(\n",
        "    \"mongodb://mongo:xQxzXZEzUilnKKhrbELE@containers-us-west-114.railway.app:6200\"\n",
        ")\n",
        "database = client[\"users\"]\n",
        "collection = database[\"recipients\"]\n",
        "# first make a set to avoid getting identical phone numbers:\n",
        "phone_numbers = set()\n",
        "recipients = collection.find()\n",
        "for recipient in recipients:\n",
        "    phone_number = recipient[\"phone_number\"]\n",
        "    phone_numbers.add(phone_number)\n",
        "# convert back to list\n",
        "phone_numbers = list(phone_numbers)\n",
        "\n",
        "########################        STEP TWO            ################################\n",
        "# setting up the vectorstore\n",
        "embeddings = OpenAIEmbeddings()\n",
        "pinecone.init(\n",
        "    api_key=os.environ.get(\"PINECONE_API_KEY\"),\n",
        "    environment=\"northamerica-northeast1-gcp\",\n",
        ")\n",
        "index = pinecone.Index(index_name=\"thematrix\")\n",
        "vectorstore = Pinecone(index, embeddings.embed_query, \"text\")\n",
        "\n",
        "\n",
        "# semantic memories for each user\n",
        "for phone_number in phone_numbers:\n",
        "    history = MongoDBChatMessageHistory(\n",
        "        connection_string=\"mongodb://mongo:xQxzXZEzUilnKKhrbELE@containers-us-west-114.railway.app:6200\",\n",
        "        session_id=phone_number,\n",
        "        database_name=\"test\",\n",
        "        collection_name=\"message_store\",\n",
        "    )\n",
        "    entire_history = str(history.messages).replace(\n",
        "        \", additional_kwargs={}, example=False\", \"\"\n",
        "    )\n",
        "    entire_history.replace(\"content=\", \"\")\n",
        "    char_text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000, chunk_overlap=100\n",
        "    )\n",
        "\n",
        "    doc_texts = char_text_splitter.split_text(entire_history)\n",
        "    vectorstore.add_texts(texts=doc_texts, namespace=phone_number)\n",
        "    print(f\"created semantic memories for: {phone_number}/n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import codecs\n",
        "from pymongo import MongoClient\n",
        "from langchain.memory import MongoDBChatMessageHistory\n",
        "client = MongoClient(\n",
        "    \"mongodb://mongo:xQxzXZEzUilnKKhrbELE@containers-us-west-114.railway.app:6200\"\n",
        ")\n",
        "database = client[\"users\"]\n",
        "collection = database[\"recipients\"]\n",
        "# first make a set to avoid getting identical phone numbers:\n",
        "phone_numbers = set()\n",
        "recipients = collection.find()\n",
        "for recipient in recipients:\n",
        "    phone_number = recipient[\"phone_number\"]\n",
        "    phone_numbers.add(phone_number)\n",
        "# convert back to list\n",
        "phone_numbers = list(phone_numbers)\n",
        "\n",
        "data_file = open(file='whole_data.txt', mode='wb')\n",
        "data_file.close()\n",
        "\n",
        "\n",
        "\n",
        "for phone_number in phone_numbers:  \n",
        "    history = MongoDBChatMessageHistory(\n",
        "        connection_string=\"mongodb://mongo:xQxzXZEzUilnKKhrbELE@containers-us-west-114.railway.app:6200\",\n",
        "        session_id=phone_number,\n",
        "        database_name=\"test\",\n",
        "        collection_name=\"message_store\",\n",
        "    )\n",
        "    entire_history = str(history.messages).replace(\n",
        "        \", additional_kwargs={}, example=False\", \"\"\n",
        "    )\n",
        "    entire_history = entire_history.replace(\"content=\", \"\")\n",
        "    with codecs.open(filename=\"whole_data.txt\", mode=\"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(entire_history)\n",
        "        f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "empty vocabulary; perhaps the documents only contain stop words",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[31], line 78\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[39m# Apply text clustering\u001b[39;00m\n\u001b[0;32m     77\u001b[0m num_clusters \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[1;32m---> 78\u001b[0m cluster_labels \u001b[39m=\u001b[39m apply_text_clustering(filtered_queries, num_clusters)\n\u001b[0;32m     80\u001b[0m \u001b[39m# Set a threshold and filter questions\u001b[39;00m\n\u001b[0;32m     81\u001b[0m threshold \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
            "Cell \u001b[1;32mIn[31], line 35\u001b[0m, in \u001b[0;36mapply_text_clustering\u001b[1;34m(user_queries, num_clusters)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_text_clustering\u001b[39m(user_queries, num_clusters):\n\u001b[0;32m     34\u001b[0m     vectorizer \u001b[39m=\u001b[39m TfidfVectorizer()\n\u001b[1;32m---> 35\u001b[0m     vectors \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39;49mfit_transform(user_queries)\n\u001b[0;32m     36\u001b[0m     kmeans \u001b[39m=\u001b[39m KMeans(n_clusters\u001b[39m=\u001b[39mnum_clusters)\n\u001b[0;32m     37\u001b[0m     kmeans\u001b[39m.\u001b[39mfit(vectors)\n",
            "File \u001b[1;32mc:\\Users\\tarim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2133\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2126\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[0;32m   2127\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2128\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[0;32m   2129\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[0;32m   2130\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[0;32m   2131\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[0;32m   2132\u001b[0m )\n\u001b[1;32m-> 2133\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[0;32m   2134\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[0;32m   2135\u001b[0m \u001b[39m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2136\u001b[0m \u001b[39m# we set copy to False\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\tarim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1380\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1381\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m             )\n\u001b[0;32m   1386\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1390\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1391\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\tarim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1294\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1292\u001b[0m     vocabulary \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1293\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1294\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1295\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1296\u001b[0m         )\n\u001b[0;32m   1298\u001b[0m \u001b[39mif\u001b[39;00m indptr[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m np\u001b[39m.\u001b[39miinfo(np\u001b[39m.\u001b[39mint32)\u001b[39m.\u001b[39mmax:  \u001b[39m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1299\u001b[0m     \u001b[39mif\u001b[39;00m _IS_32BIT:\n",
            "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
          ]
        }
      ],
      "source": [
        "import codecs\n",
        "import re\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Preprocess the chat data\n",
        "def preprocess_chat_data(chat_data):\n",
        "    queries = re.findall(r'HumanMessage\\(\"([^\"]+)\"\\)', chat_data)\n",
        "    return queries\n",
        "\n",
        "# Extract user queries\n",
        "def extract_user_queries(sentences, user_marker='User:'):\n",
        "    user_queries = []\n",
        "    for sentence in sentences:\n",
        "        if sentence.startswith(user_marker):\n",
        "            query = sentence[len(user_marker):].strip()\n",
        "            user_queries.append(query)\n",
        "    return user_queries\n",
        "\n",
        "# Build frequency distribution\n",
        "def build_frequency_distribution(user_queries):\n",
        "    freq_distribution = Counter(user_queries)\n",
        "    return freq_distribution\n",
        "\n",
        "# Remove stopwords\n",
        "def remove_stopwords(user_queries):\n",
        "    return user_queries\n",
        "\n",
        "\n",
        "# Apply text clustering\n",
        "def apply_text_clustering(user_queries, num_clusters):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    vectors = vectorizer.fit_transform(user_queries)\n",
        "    kmeans = KMeans(n_clusters=num_clusters)\n",
        "    kmeans.fit(vectors)\n",
        "    cluster_labels = kmeans.labels_\n",
        "    return cluster_labels\n",
        "\n",
        "# Set a threshold and filter questions\n",
        "def filter_questions(user_queries, freq_distribution, cluster_labels, threshold):\n",
        "    filtered_questions = []\n",
        "    for i, query in enumerate(user_queries):\n",
        "        freq = freq_distribution[query]\n",
        "        cluster_label = cluster_labels[i]\n",
        "        if freq > threshold and cluster_label == 0:\n",
        "            filtered_questions.append(query)\n",
        "    return filtered_questions\n",
        "\n",
        "# Review and refine the identified questions\n",
        "def review_and_refine(filtered_questions):\n",
        "    # Additional logic for reviewing and refining the questions\n",
        "    refined_questions = filtered_questions\n",
        "    return refined_questions\n",
        "\n",
        "# Example usage\n",
        "def chat_data():\n",
        "    with codecs.open(filename=\"whole_data.txt\", mode=\"r\", encoding=\"utf-8\") as f:\n",
        "        data = f.read()\n",
        "        f.close()\n",
        "    return data\n",
        "\n",
        "# Preprocess chat data\n",
        "sentences = preprocess_chat_data(chat_data())\n",
        "\n",
        "# Extract user queries\n",
        "user_queries = extract_user_queries(sentences)\n",
        "\n",
        "# Build frequency distribution\n",
        "freq_distribution = build_frequency_distribution(user_queries)\n",
        "\n",
        "# Remove stopwords\n",
        "filtered_queries = remove_stopwords(user_queries)\n",
        "\n",
        "# Apply text clustering\n",
        "num_clusters = 3\n",
        "cluster_labels = apply_text_clustering(filtered_queries, num_clusters)\n",
        "\n",
        "# Set a threshold and filter questions\n",
        "threshold = 1\n",
        "filtered_questions = filter_questions(filtered_queries, freq_distribution, cluster_labels, threshold)\n",
        "\n",
        "# Review and refine questions\n",
        "refined_questions = review_and_refine(filtered_questions)\n",
        "\n",
        "# Print the refined questions\n",
        "print(\"Refined Questions:\")\n",
        "for question in refined_questions:\n",
        "    print(question)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def remove_stopwords(user_queries):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_queries = []\n",
        "    for query in user_queries:\n",
        "        words = word_tokenize(query)\n",
        "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "        filtered_query = ' '.join(filtered_words)\n",
        "        if filtered_query:\n",
        "            filtered_queries.append(filtered_query)\n",
        "    return filtered_queries\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "custom_stopwords = ['a', 'is', 'and','the','in',  'ðŸ¤”']  # Add stopwords and special characters as needed\n",
        "\n",
        "def remove_stopwords(user_queries):\n",
        "    filtered_queries = []\n",
        "    for query in user_queries:\n",
        "        words = query.split()\n",
        "        filtered_words = [word for word in words if word.lower() not in custom_stopwords]\n",
        "        filtered_query = ' '.join(filtered_words)\n",
        "        if filtered_query:\n",
        "            filtered_queries.append(filtered_query)\n",
        "    return filtered_queries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "user_queries = extract_user_queries(sentences)\n",
        "remove_stopwords(user_queries=user_queries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Counter()"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "freq_distribution = build_frequency_distribution(user_queries)\n",
        "freq_distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "filtered_queries = remove_stopwords(user_queries)\n",
        "filtered_queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template_one = PromptTemplate(\n",
        "    input_variables=['industry'],\n",
        "    template= \"create a name for a {industry} company.\"\n",
        ")\n",
        "template_two = PromptTemplate(\n",
        "    input_variables=[\"name\"],\n",
        "    template=\"write a catchphrase for the company {name}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI()\n",
        "llm_chain_1 = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=template_one\n",
        ")\n",
        "llm_chain_2 = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=template_two\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chains import SimpleSequentialChain\n",
        "\n",
        "seq = SimpleSequentialChain(chains=[llm_chain_1, llm_chain_2])\n",
        "\n",
        "seq.run(\"tech\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
